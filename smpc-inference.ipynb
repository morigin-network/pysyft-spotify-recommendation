{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMPC idea for model inference\n",
    "\n",
    "This notebook shows the overall flow of model inferencing with SMPC.  \n",
    "Assumptions made:  \n",
    "1. Two workers will be performing the computation.  Pygrid and the data holder's device will not participate in any computation, they are just responsible for data splitting and sending to the workers.  \n",
    "2. No worker will get a full copy of unencrypted user data nor the model weights. Each of them will get only an encrypted tensor which could assemble the original data only when the tensors from different devices are combined together. \n",
    "3. Both workers will be able to get the basic model architecture and initialize it with random weights (i.e. Assume that only the weights are sensitive but not the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this if needed\n",
    "#!pip install crypten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import torch as th\n",
    "import collections\n",
    "\n",
    "crypten.init()\n",
    "th.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define different hosts participating in SMPC\n",
    "WORKER1 = 0\n",
    "WORKER2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters (this will be )\n",
    "n_users = 10\n",
    "song_features = 10\n",
    "bs = 64  # batch_size\n",
    "lr = 5e-4\n",
    "embedding_size = 50\n",
    "layer_sizes = [(embedding_size + song_features, 150), (150, 300), (300, 200), (200, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 -- User data split\n",
    "\n",
    "The following are assumed to be run on user device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User data generation -- assume the data below are from the user device\n",
    "user = th.zeros((n_users,))\n",
    "user[0] = 1\n",
    "user = th.reshape(user,(1,-1))\n",
    "features = th.tensor([0.233021,1.320897,0.128987,0.613270,0.256593,-0.377054,0.104040,0.394174,-0.239261,-0.550271])\n",
    "features = th.reshape(features,(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hinnes/.pyenv/versions/3.8.12/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/hinnes/.pyenv/versions/3.8.12/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/hinnes/.pyenv/versions/3.8.12/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hinnes/.pyenv/versions/3.8.12/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hinnes/projects/CrypTen/crypten/mpc/context.py\", line 30, in _launch\n",
      "    return_value = func(*func_args, **func_kwargs)\n",
      "  File \"/home/hinnes/projects/CrypTen/crypten/mpc/context.py\", line 30, in _launch\n",
      "    return_value = func(*func_args, **func_kwargs)\n",
      "  File \"/tmp/ipykernel_23033/236583654.py\", line 9, in data_separation\n",
      "    crypten.save(usertensor, f\"testdata/user{rank}.pth\")\n",
      "  File \"/tmp/ipykernel_23033/236583654.py\", line 9, in data_separation\n",
      "    crypten.save(usertensor, f\"testdata/user{rank}.pth\")\n",
      "  File \"/home/hinnes/projects/CrypTen/crypten/__init__.py\", line 443, in save\n",
      "    save_closure(obj, f, **kwargs)\n",
      "  File \"/home/hinnes/projects/CrypTen/crypten/__init__.py\", line 443, in save\n",
      "    save_closure(obj, f, **kwargs)\n",
      "  File \"/home/hinnes/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py\", line 369, in save\n",
      "    with _open_file_like(f, 'wb') as opened_file:\n",
      "  File \"/home/hinnes/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py\", line 369, in save\n",
      "    with _open_file_like(f, 'wb') as opened_file:\n",
      "  File \"/home/hinnes/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py\", line 230, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/hinnes/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py\", line 230, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/hinnes/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py\", line 211, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "  File \"/home/hinnes/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py\", line 211, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'testdata/user0.pth'\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'testdata/user1.pth'\n",
      "ERROR:root:One of the parties failed. Check past logs\n"
     ]
    }
   ],
   "source": [
    "from crypten import mpc\n",
    "import crypten.communicator as comm \n",
    "@mpc.run_multiprocess(world_size=2) #the world_size here corresponds to how many workers running the SMPC\n",
    "def data_separation():\n",
    "    usertensor = crypten.cryptensor(user)\n",
    "    featuretensor = crypten.cryptensor(features)\n",
    "    rank = comm.get().get_rank()\n",
    "    #crypten.print(f\"\\nRank {rank}:\\n {usertensor}\\n\", in_order=True)\n",
    "    crypten.save(usertensor, f\"data/user{rank}.pth\") \n",
    "    crypten.save(featuretensor,f\"data/feature{rank}.pth\")\n",
    "data_separation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 -- Model split\n",
    "\n",
    "The following are assumed to be run on pygrid / parcel (#TODO: check if it is possible to obtain the encrypted split weights directly in parcel and save to pygrid?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: how to save this model architecture in pygrid and send it to both workers (unencrypted, only random weights)\n",
    "#NOTE THAT the model definition is slightly different from the one used in pygrid mobile, but should still work \n",
    "#since the shape of model weights remain the same (i.e. we could still copy the weights from the trained model in pygrid to here)\n",
    "class EmbeddingNet(th.nn.Module):  \n",
    "    \"\"\"\n",
    "    Simple model with method for loss and hand-written backprop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.embedlayer = th.nn.Linear(n_users, embedding_size) #chagned\n",
    "        self.fc1 = th.nn.Linear(layer_sizes[0][0], layer_sizes[0][1])\n",
    "        self.fc2 = th.nn.Linear(layer_sizes[1][0], layer_sizes[1][1])\n",
    "        self.fc3 = th.nn.Linear(layer_sizes[2][0], layer_sizes[2][1])\n",
    "        self.fc4 = th.nn.Linear(layer_sizes[3][0], layer_sizes[3][1])\n",
    "\n",
    "    def forward(self, users,features):\n",
    "        \"\"\"\n",
    "        users: a one-hot tensor of size (n_users,) representing the user.\n",
    "        features: 10d vector using spotify-provided feature values\n",
    "        x: a 60d dummy vector required from user\n",
    "        \"\"\"\n",
    "        out = self.embedlayer(users)\n",
    "        out = th.cat((out,features),dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = th.nn.functional.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = th.nn.functional.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = th.nn.functional.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = th.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'testdata/model_random.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hinnes/projects/pysyft-spotify-recommendation/smpc-inference.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hinnes/projects/pysyft-spotify-recommendation/smpc-inference.ipynb#ch0000011vscode-remote?line=1'>2</a>\u001b[0m crypten\u001b[39m.\u001b[39mcommon\u001b[39m.\u001b[39mserial\u001b[39m.\u001b[39mregister_safe_class(EmbeddingNet)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hinnes/projects/pysyft-spotify-recommendation/smpc-inference.ipynb#ch0000011vscode-remote?line=2'>3</a>\u001b[0m local_model \u001b[39m=\u001b[39m EmbeddingNet()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hinnes/projects/pysyft-spotify-recommendation/smpc-inference.ipynb#ch0000011vscode-remote?line=3'>4</a>\u001b[0m th\u001b[39m.\u001b[39;49msave(local_model,\u001b[39m\"\u001b[39;49m\u001b[39mtestdata/model_random.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hinnes/projects/pysyft-spotify-recommendation/smpc-inference.ipynb#ch0000011vscode-remote?line=4'>5</a>\u001b[0m \u001b[39m#Save trained model : Weights are dummy for the purpose of this demo\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hinnes/projects/pysyft-spotify-recommendation/smpc-inference.ipynb#ch0000011vscode-remote?line=5'>6</a>\u001b[0m trained_model \u001b[39m=\u001b[39m EmbeddingNet()\n",
      "File \u001b[0;32m~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py:369\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=334'>335</a>\u001b[0m \u001b[39m\"\"\"Saves an object to a disk file.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=335'>336</a>\u001b[0m \n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=336'>337</a>\u001b[0m \u001b[39mSee also: `saving-loading-tensors`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=364'>365</a>\u001b[0m \u001b[39m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=365'>366</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=366'>367</a>\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m--> <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=368'>369</a>\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=369'>370</a>\u001b[0m     \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=370'>371</a>\u001b[0m         \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[0;32m~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=227'>228</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=228'>229</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=229'>230</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=230'>231</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=231'>232</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=209'>210</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> <a href='file:///~/projects/pysyft-spotify-recommendation/env/lib/python3.8/site-packages/torch/serialization.py?line=210'>211</a>\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'testdata/model_random.pth'"
     ]
    }
   ],
   "source": [
    "#save a dummy model with random weights\n",
    "crypten.common.serial.register_safe_class(EmbeddingNet)\n",
    "local_model = EmbeddingNet()\n",
    "th.save(local_model,\"data/model_random.pth\")\n",
    "#Save trained model : Weights are dummy for the purpose of this demo\n",
    "trained_model = EmbeddingNet()\n",
    "sd = trained_model.state_dict()\n",
    "for layer in sd:\n",
    "    sd[layer] = th.ones(sd[layer].size())\n",
    "trained_model.load_state_dict(sd)\n",
    "th.save(trained_model,\"data/model_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def model_split():\n",
    "    #Load the model\n",
    "    plaintext_model = th.load('data/model_trained.pth')\n",
    "    \n",
    "    #Construct a CrypTen network with the trained model and dummy_input\n",
    "    dummy_input = th.empty((1,10,)) ,th.empty((1,10,))\n",
    "    private_model = crypten.nn.from_pytorch(plaintext_model, dummy_input)\n",
    "    \n",
    "    #3. Encrypt the CrypTen network \n",
    "    private_model.encrypt(src=0)\n",
    "    rank = comm.get().get_rank()\n",
    "\n",
    "    crypten.save(private_model.state_dict(), f\"data/modelweights{rank}.pth\") \n",
    "\n",
    "    # #Check that model is encrypted:\n",
    "    print(\"Model successfully encrypted:\", private_model.encrypted)\n",
    "\n",
    "model_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Run the SMPC inferencing\n",
    "\n",
    "The following is to model two workers simply by two processes existing in a single machine. I have yet to determine how this could be done in multiple machines (hopefully should be straightforward.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm data ok\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def model_inference():\n",
    "    #1. Combine user data \n",
    "    user0 = crypten.load_from_party(\"data/user0.pth\",encrypted=True,src=WORKER1)\n",
    "    user1 = crypten.load_from_party(\"data/user1.pth\",encrypted=True,src=WORKER2)\n",
    "    user = (user0+user1)\n",
    "    crypten.print(\"Tensor encrypted:\", crypten.is_encrypted_tensor(user)) \n",
    "    crypten.print(\"Decrypted data:\",user.get_plain_text())\n",
    "    feature0 = crypten.load_from_party(\"data/feature0.pth\",encrypted=True,src=WORKER1)\n",
    "    feature1 = crypten.load_from_party(\"data/feature1.pth\",encrypted=True,src=WORKER2)\n",
    "    feature = (feature0+feature1)\n",
    "    crypten.print(\"Tensor encrypted:\", crypten.is_encrypted_tensor(feature)) \n",
    "    crypten.print(\"Decrypted data:\",feature.get_plain_text())\n",
    "    #2. load basic model\n",
    "    plaintext_model = th.load('data/model_random.pth') #use of load method instead of load_from_party: meaning that both process should have a copy of the document.\n",
    "    dummy_input = th.empty((1,10,)), th.empty((1,10,))\n",
    "    private_model = crypten.nn.from_pytorch(plaintext_model, dummy_input)\n",
    "    private_model.encrypt(src=WORKER1) #Actually the src is not important, weights are just dummy\n",
    "    #3. load and combine model weights\n",
    "    w1 = crypten.load_from_party(\"data/modelweights0.pth\",encrypted=True,src=WORKER1)\n",
    "    w2 = crypten.load_from_party(\"data/modelweights1.pth\",encrypted=True,src=WORKER2)\n",
    "    rank = comm.get().get_rank()\n",
    "    for k in w1:\n",
    "        #Crypten strangely only supports enc + plain but does not support the other way round (plain + enc)\n",
    "        if rank == WORKER1:\n",
    "            w1[k] = w1[k] + w2[k]\n",
    "        else:\n",
    "            w2[k] = w2[k] + w1[k]\n",
    "    #4. restore weights to basic model\n",
    "    if rank == WORKER1:\n",
    "        private_model.load_state_dict(w1)\n",
    "    elif rank == WORKER2:\n",
    "        private_model.load_state_dict(w2)\n",
    "    #5. inference\n",
    "    private_model.eval()\n",
    "    output_enc = private_model(user,feature)\n",
    "    crypten.print(output_enc.get_plain_text())\n",
    "    #Sanity check the model weights\n",
    "    # private_model.decrypt()\n",
    "    # print(private_model.state_dict())\n",
    "model_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO\n",
    "1. complete this poc and confirm things are right\n",
    "2. try deploy this in two separate machines\n",
    "3. (see if ok) Modify PyGrid to give teh  encrypted tensors directly so even pygrid no need see the data?\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "694cb5ecd12f722d95921d5c5a312ddba03d00263a951b715d8b83b38cc41a70"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('crypten': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
