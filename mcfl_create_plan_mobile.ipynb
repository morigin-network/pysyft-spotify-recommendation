{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model-Centric Federated Learning for Mobile - Spotify Recommendation\n",
    "\n",
    "This notebook is adapted from the PySyft model-centric federated learning [MNIST example](https://github.com/OpenMined/PySyft/blob/syft_0.5.0/packages/syft/examples/federated-learning/model-centric/mcfl_create_plan_mobile.ipynb), modified for training a Spotify track recommendation model.\n",
    "\n",
    "This is a simple mode. It will take in a user index and some features of the tracks such as tempo, and predict whether the user likes the track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import base64\n",
    "import json\n",
    "\n",
    "# third party\n",
    "import torch as th\n",
    "\n",
    "# syft absolute\n",
    "import syft as sy\n",
    "from syft.core.plan.plan_builder import ROOT_CLIENT\n",
    "from syft.core.plan.plan_builder import PLAN_BUILDER_VM\n",
    "from syft.core.plan.plan_builder import make_plan\n",
    "from syft.core.plan.translation.torchscript.plan_translate import (\n",
    "    translate as translate_to_ts,\n",
    ")\n",
    "from syft.federated.model_centric_fl_client import ModelCentricFLClient\n",
    "from syft.lib.python.int import Int\n",
    "from syft.lib.python.list import List\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "th.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training Configurations\n",
    "\n",
    "* n_users: number of users joining the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_users = 10\n",
    "song_features = 10\n",
    "bs = 64  # batch_size\n",
    "lr = 5e-4\n",
    "embedding_size = 50\n",
    "# Number of layer is hard-coded, any changes on no. of layers would require change of the EmbeddingNet class too.\n",
    "layer_sizes = [(embedding_size + song_features, 150), (150, 300), (300, 200), (200, 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the model\n",
    "\n",
    "Let's create a embedding net, which contains an embedding layer embedding user identify, and a fully connected net that has concat of embedding output and the song features as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(\n",
    "    sy.Module\n",
    "):  # This version follows hinnes' implmentation (first assume only one user and using numeric attributes of the song)\n",
    "    \"\"\"\n",
    "    Simple model with method for loss and hand-written backprop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, torch_ref) -> None:\n",
    "        super(EmbeddingNet, self).__init__(torch_ref=torch_ref)\n",
    "        self.torch_ref = torch_ref\n",
    "        self.embedlayer = torch_ref.nn.Linear(n_users, embedding_size)\n",
    "        torch_ref.nn.init.constant_(self.embedlayer.bias.data, 0)\n",
    "        self.embeddrop = torch_ref.nn.Dropout(0.02)\n",
    "        self.fc1 = torch_ref.nn.Linear(layer_sizes[0][0], layer_sizes[0][1])\n",
    "        self.a1 = torch_ref.nn.ReLU()\n",
    "        self.d1 = torch_ref.nn.Dropout(0.3)\n",
    "        self.fc2 = torch_ref.nn.Linear(layer_sizes[1][0], layer_sizes[1][1])\n",
    "        self.a2 = torch_ref.nn.ReLU()\n",
    "        self.d2 = torch_ref.nn.Dropout(0.3)\n",
    "        self.fc3 = torch_ref.nn.Linear(layer_sizes[2][0], layer_sizes[2][1])\n",
    "        self.a3 = torch_ref.nn.ReLU()\n",
    "        self.fc4 = torch_ref.nn.Linear(layer_sizes[3][0], layer_sizes[3][1])\n",
    "\n",
    "    def forward(self, users, features, x):\n",
    "        \"\"\"\n",
    "        users: a one-hot tensor of size (n_users,) representing the user.\n",
    "        features: 10d vector using spotify-provided feature values\n",
    "        x: a 60d dummy vector required from user\n",
    "        \"\"\"\n",
    "        self.embedout = self.embedlayer(users.float())\n",
    "        self.embedout = self.embeddrop(self.embedout)\n",
    "        x[:, :embedding_size] = self.embedout\n",
    "        x[:, embedding_size : embedding_size + song_features] = features\n",
    "        self.catout = x\n",
    "        x = self.fc1(self.catout)\n",
    "        x = self.a1(x)\n",
    "        self.l1out = self.d1(x)\n",
    "        y = self.fc2(self.l1out)\n",
    "        y = self.a2(y)\n",
    "        self.l2out = self.d2(y)\n",
    "        z = self.fc3(self.l2out)\n",
    "        self.l3out = self.a3(z)\n",
    "        k = self.fc4(self.l3out)\n",
    "        self.l4out = self.torch_ref.sigmoid(k)\n",
    "        return self.l4out\n",
    "\n",
    "    def backward(self, user, error):  # BATCH_SIZE=1 is assumed.\n",
    "        pgs, flayers, aouts = [], [], []\n",
    "        latest_grad = error\n",
    "        aouts.extend([self.catout, self.l1out, self.l2out, self.l3out])\n",
    "        flayers.extend([self.fc2, self.fc3, self.fc4])\n",
    "        for i in range(len(aouts)):\n",
    "            j = len(aouts) - i - 1\n",
    "            pgs.append(latest_grad.sum(0))  # bias grad\n",
    "            pgs.append(latest_grad.t() @ aouts[j])  # weight grad\n",
    "            if j - 1 >= 0:\n",
    "                latest_grad = (latest_grad @ flayers[j - 1].state_dict()[\"weight\"]) * (\n",
    "                    aouts[j] > 0\n",
    "                ).float()\n",
    "            else:  # Embedding layer\n",
    "                latest_grad = (\n",
    "                    latest_grad @ self.fc1.state_dict()[\"weight\"]\n",
    "                )  # no ReLU in embedding\n",
    "                latest_grad = latest_grad[:, :embedding_size]\n",
    "        embedgrad = latest_grad.t() @ user.float()\n",
    "        # For embedding layer, we mimic using a linear layer.\n",
    "        # Therefore, bias exists but we do not need it.\n",
    "        # Hence providing a zero tensor so that the bias weight remains 0.\n",
    "        pgs.append(th.zeros((embedding_size,)))\n",
    "        pgs.append(embedgrad)\n",
    "        pgs.reverse()\n",
    "        return tuple(pgs)\n",
    "\n",
    "    def mse_loss(self, fpass, target):  # MSE first\n",
    "        squared_error = self.torch_ref.pow(fpass - target, 2)\n",
    "        loss = squared_error.mean()\n",
    "        loss_grad = (target - fpass) * fpass * (1 - fpass) / bs\n",
    "        return loss, loss_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingNet(th)\n",
    "\n",
    "features = th.randn(bs, song_features)\n",
    "users = th.randint(1, n_users, (bs,))\n",
    "users = th.nn.functional.one_hot(users)\n",
    "x = th.randn(bs, song_features + embedding_size)\n",
    "fpass = model(users, features, x)\n",
    "batch_size = th.tensor([bs])\n",
    "ys = th.rand(bs, 1)\n",
    "loss, loss_grad = model.mse_loss(fpass, ys)\n",
    "print(\"Results here:\")\n",
    "grads = model.backward(users, loss_grad)\n",
    "# print(grads.shape)\n",
    "for x in grads:\n",
    "    print(x.shape)\n",
    "print(\"Results sdf here:\")\n",
    "for y in model.parameters():\n",
    "    print(y.shape)\n",
    "# updated_params = tuple(\n",
    "#         param - lr * grad for param, grad in zip(model.get_model_parameters(), grads)\n",
    "#     )\n",
    "# print(model.fc2.state_dict()[\"weight\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Training Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_remote_model_params(module_ptrs, params_list_ptr):\n",
    "    \"\"\"Sets the model parameters into traced model\"\"\"\n",
    "    param_idx = 0\n",
    "    for module_name, module_ptr in module_ptrs.items():\n",
    "        for param_name, _ in PLAN_BUILDER_VM.store[\n",
    "            module_ptr.id_at_location\n",
    "        ].data.named_parameters():\n",
    "            module_ptr.register_parameter(param_name, params_list_ptr[param_idx])\n",
    "            param_idx += 1\n",
    "\n",
    "# Create the model\n",
    "local_model = EmbeddingNet(th)\n",
    "\n",
    "\n",
    "model_params_zeros = sy.lib.python.List(\n",
    "    [th.nn.Parameter(th.zeros_like(param)) for param in local_model.parameters()]\n",
    ")\n",
    "\n",
    "@make_plan\n",
    "def training_plan(\n",
    "    features = th.randn(bs,10),\n",
    "    users = th.nn.functional.one_hot(th.randint(1,n_users,(bs,))),\n",
    "    xs = th.randn(bs,embedding_size+song_features),\n",
    "    ys=th.rand(bs,1),\n",
    "    lr=th.tensor([lr]),\n",
    "    params=model_params_zeros,\n",
    "):\n",
    "    # send the model to plan builder (but not its default params)\n",
    "    # this is required to build the model inside the Plan\n",
    "    model = local_model.send(ROOT_CLIENT, send_parameters=False)\n",
    "\n",
    "    # set model params from input\n",
    "    set_remote_model_params(model.modules, params)\n",
    "    \n",
    "\n",
    "    # forward\n",
    "    fpass = model(users,features,xs)\n",
    "\n",
    "    # loss\n",
    "    loss, loss_grad = model.mse_loss(fpass,ys)\n",
    "\n",
    "    # backward\n",
    "    grads = model.backward(users, loss_grad)\n",
    "\n",
    "    # SGD step\n",
    "    updated_params = tuple(\n",
    "        param - lr * grad for param, grad in zip(model.parameters(), grads)\n",
    "    )\n",
    "\n",
    "    # return things\n",
    "    return (loss, *updated_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the training plan to torchscript so it can be used with mobile workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Translate to torchscript\n",
    "ts_plan = translate_to_ts(training_plan)\n",
    "\n",
    "# Let's examine its contents\n",
    "print(ts_plan.torchscript.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Averaging Plan\n",
    "\n",
    "Averaging Plan is executed by PyGrid at the end of the cycle,\n",
    "to average _diffs_ submitted by workers and update the model\n",
    "and create new checkpoint for the next cycle.\n",
    "\n",
    "_Diff_ is the difference between client-trained\n",
    "model params and original model params,\n",
    "so it has same number of tensors and tensor's shapes\n",
    "as the model parameters.\n",
    "\n",
    "We define Plan that processes one diff at a time.\n",
    "Such Plans require `iterative_plan` flag set to `True`\n",
    "in `server_config` when hosting FL model to PyGrid.\n",
    "\n",
    "Plan below will calculate simple mean of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@make_plan\n",
    "def avg_plan(\n",
    "    avg=List(local_model.parameters()), item=List(local_model.parameters()), num=Int(0)\n",
    "):\n",
    "    new_avg = []\n",
    "    for i, param in enumerate(avg):\n",
    "        new_avg.append((avg[i] * num + item[i]) / (num + 1))\n",
    "    return new_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Federated Learning Process Configuration\n",
    "\n",
    "Before hosting the model and training plan to PyGrid,\n",
    "we need to define some configuration parameters, such as\n",
    "FL process name, version, workers configuration,\n",
    "authentication method, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "name = \"spotify_recommendation\"\n",
    "version = \"1.0\"\n",
    "\n",
    "client_config = {\n",
    "    \"name\": name,\n",
    "    \"version\": version,\n",
    "    \"batch_size\": bs,\n",
    "    \"lr\": lr,\n",
    "    \"max_updates\": 100,  # custom syft.js option that limits number of training loops per worker\n",
    "}\n",
    "\n",
    "server_config = {\n",
    "    # \"min_workers\": 2,\n",
    "    # \"max_workers\": 2,\n",
    "    # \"pool_selection\": \"random\",\n",
    "    # \"do_not_reuse_workers_until_cycle\": 6,\n",
    "    \"cycle_length\": 28800,  # max cycle length in seconds\n",
    "    \"num_cycles\": 30,  # max number of cycles\n",
    "    \"max_diffs\": 1,  # number of diffs to collect before avg\n",
    "    \"minimum_upload_speed\": 0,\n",
    "    \"minimum_download_speed\": 0,\n",
    "    \"iterative_plan\": True,  # tells PyGrid that avg plan is executed per diff\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This FL process will require workers to authenticate with signed JWT token.\n",
    "Providing the `pub_key` in FL configuration allows PyGrid to verify JWT tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(fname):\n",
    "    with open(fname, \"r\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "public_key = read_file(\"example_rsa.pub\").strip()\n",
    "\n",
    "server_config[\"authentication\"] = {\n",
    "    \"type\": \"jwt\",\n",
    "    \"pub_key\": public_key,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Host in PyGrid\n",
    "\n",
    "Let's now host everything in PyGrid so that it can be accessed by worker libraries.\n",
    "\n",
    "Note: assuming the PyGrid Domain is running locally on port 7001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grid_address = \"localhost:7001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the domain\n",
    "\n",
    "Run this once and only once after the PyGrid is cleared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.grid.client.client import connect\n",
    "from syft.grid.client.grid_connection import GridHTTPConnection\n",
    "\n",
    "domain = connect(\n",
    "    url=f\"http://{grid_address}\", \n",
    "    conn_type=GridHTTPConnection,\n",
    ")\n",
    "domain.setup(\n",
    "    email=\"owner@openmined.org\",\n",
    "    password=\"owerpwd\",\n",
    "    domain_name=\"OpenMined Node\",\n",
    "    token=\"9G9MJ06OQH\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ModelCentricFLClient(address=grid_address, secure=False)\n",
    "domain = grid.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code sends FL model, training plans, and configuration to the PyGrid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "response = grid.host_federated_training(\n",
    "    model=local_model,\n",
    "    client_plans={\n",
    "        # Grid can store both types of plans (regular for python worker, torchscript for mobile):\n",
    "        \"training_plan\": training_plan,\n",
    "        \"training_plan:ts\": ts_plan,\n",
    "    },\n",
    "    client_protocols={},\n",
    "    server_averaging_plan=avg_plan,\n",
    "    client_config=client_config,\n",
    "    server_config=server_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see successful response, you've just hosted your first FL process into PyGrid!\n",
    "\n",
    "If you see error that FL process already exists,\n",
    "this means FL process with such name and version is already hosted.\n",
    "You might want to update name/version in configuration above, or cleanup PyGrid database.\n",
    "\n",
    "To cleanup database, set path below correctly and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !rm PyGrid/apps/domain/src/nodedatabase.db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To train hosted model, use one of the existing mobile FL workers:\n",
    " * [SwiftSyft](https://github.com/OpenMined/SwiftSyft) (see included worker example)\n",
    " * [KotlinSyft](https://github.com/OpenMined/KotlinSyft) (see included worker example)\n",
    "\n",
    "Support for javascript worker is coming soon:\n",
    " * [syft.js](https://github.com/OpenMined/syft.js)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
